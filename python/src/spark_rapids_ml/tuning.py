#
# Copyright (c) 2023, NVIDIA CORPORATION.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
from multiprocessing.pool import ThreadPool
from typing import cast, List, Sequence, Callable, Tuple

import numpy as np
from pyspark import inheritable_thread_target
from pyspark.ml import Model, Estimator, Transformer
from pyspark.ml.evaluation import Evaluator
from pyspark.ml.tuning import CrossValidator as SparkCrossValidator, CrossValidatorModel
from pyspark.sql import DataFrame


def _parallelFitTasks(
    est: Estimator,
    train: Sequence[DataFrame],
    eva: Evaluator,
    validation: Sequence[DataFrame],
    epm: Sequence["ParamMap"],
    collectSubModel: bool,
) -> List[Callable[[], Tuple[int, float, Transformer]]]:
    """
    Creates a list of callables which can be called from different threads to fit and evaluate
    an estimator in parallel. Each callable returns an `(index, metric)` pair.

    Parameters
    ----------
    est : :py:class:`pyspark.ml.baseEstimator`
        he estimator to be fit.
    train : :py:class:`pyspark.sql.DataFrame`
        DataFrame, training data set, used for fitting.
    eva : :py:class:`pyspark.ml.evaluation.Evaluator`
        used to compute `metric`
    validation : :py:class:`pyspark.sql.DataFrame`
        DataFrame, validation data set, used for evaluation.
    epm : :py:class:`collections.abc.Sequence`
        Sequence of ParamMap, params maps to be used during fitting & evaluation.
    collectSubModel : bool
        Whether to collect sub model.

    Returns
    -------
    tuple
        (int, float, subModel), an index into `epm` and the associated metric value.
    """
    def singleTask() -> Tuple[int, float, Transformer]:
        modelIter = est.fitMultiple(train, epm)

        index, model = next(modelIter)
        # TODO: duplicate evaluator to take extra params from input
        #  Note: Supporting tuning params in evaluator need update method
        #  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return
        #  all nested stages and evaluators
        metric = eva.evaluate(model.transform(validation, epm[index]))
        return index, metric, model if collectSubModel else None

    return [singleTask] * len(epm)


class CrossValidator(SparkCrossValidator):

    def _fit(self, dataset: DataFrame) -> "CrossValidatorModel":
        est = self.getOrDefault(self.estimator)
        epm = self.getOrDefault(self.estimatorParamMaps)
        numModels = len(epm)
        eva = self.getOrDefault(self.evaluator)
        nFolds = self.getOrDefault(self.numFolds)
        metrics_all = [[0.0] * numModels for i in range(nFolds)]

        # We're parallelizing the tasks according the datasets generated by k-Folds
        pool = ThreadPool(processes=min(self.getParallelism(), nFolds))
        subModels = None
        collectSubModelsParam = self.getCollectSubModels()
        if collectSubModelsParam:
            subModels = [[None for j in range(numModels)] for i in range(nFolds)]

        datasets = self._kFold(dataset)

        train_datasets = [train for train, _ in datasets]
        validation_datasets = [validation for _, validation in datasets]

        tasks = map(
            inheritable_thread_target,
            _parallelFitTasks(est, train_datasets, eva, validation_datasets, epm, collectSubModelsParam),
        )

        for j, metric, subModel in pool.imap_unordered(lambda f: f(), tasks):
            # metrics_all[i][j] = metric
            # if collectSubModelsParam:
            #     assert subModels is not None
            #     subModels[i][j] = subModel
            print(f"j: {j} metric: {metric} ")

        metrics, std_metrics = CrossValidator._gen_avg_and_std_metrics(metrics_all)

        if eva.isLargerBetter():
            bestIndex = np.argmax(metrics)
        else:
            bestIndex = np.argmin(metrics)
        bestModel = est.fit(dataset, epm[bestIndex])
        return self._copyValues(
            CrossValidatorModel(bestModel, metrics, cast(List[List[Model]], subModels), std_metrics)
        )
